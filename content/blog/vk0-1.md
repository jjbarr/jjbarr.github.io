+++
title = "Vulkan From Zero: Part 1"
description = "An overview of something I know very little about."
date = 2020-08-12
+++

[Vulkan](https://www.khronos.org/vulkan/) seems to be really exciting all the
computer graphics people, and it's easy to see why. Actually I lied: unless
you're in some way involved in the graphics scene, it's not easy at all. In
short, it's a new API that provides a lower level interface to the GPU, with
less implicit global state, and with a lot of the cruft and garbage and bad
ideas that have accumulated in OpenGL over the years forcibly ripped out. Which
is great, and if you're a certain kind of programmer you're probably already
drooling. Being the sort of person who likes clean, explicit verbosity over
magical incantations because I always have the sinking feeling that everything
is about to fuck up and I'll be suddenly tested on every element I don't
understand, I though jumping into graphics programming with Vulkan seemed like a
good idea.

Emphatically, it probably wasn't. I haven't even *reached* the point of really
understanding anything, and I already know that. Vulkan is really big, very
complicated, and most irritatingly, pretty much all of the existing resources
assume that you already know what you're doing. Which, as established, I
don't. They also assume that you want to work in C++ for the most part, which I
*also* don't, because that's just a whole other garbage fire. But if I spend all
my time trashing C++, I'll have none less for Vulkan, and [Yossi
Kreinin](http://yosefk.com/c++fqa/index.html) has more than covered that
particular base (Yes, the FQA is outdated, but it's really entertaining...).

I have a peculiar personality flaw: I hate jumping into things. So instead of
writing/copy-pasting code until I had a triangle on-screen, I *really* wanted to
understand Vulkan in principle first, and have a broad overview of how all the
parts fit together. This is hard because (as mentioned) all the docs assume you
know what you're talking about (or, in the case of the official spec, are
*painfully* verbose), and Vulkan is really, really big. Massive.

However, while I still haven't put that triangle on-screen yet (that's for the
next blog post), I've built up enough of a mental map of Vulkan to sort of
understand how the whole thing works. Kind of. Maybe. We shall see. Thus, armed
with a copy of [The Vulkan Spec
](https://www.khronos.org/registry/vulkan/specs/1.2-extensions/html/index.html),
[The Vulkan 1.1 API Quick Reference
](https://www.khronos.org/registry/vulkan/specs/1.1/refguide/Vulkan-1.1-web.pdf),
[The Vulkan Tutorial](https://vulkan-tutorial.com/), and [The Vulkan
Salute](https://en.wikipedia.org/wiki/Vulcan_salute), I shall attempt to provide
a broad overview of Vulkan, more for my own sanity than for any utilty to
anybody else.

Live Long and Prosper.

# Building Blocks

These are the small, simple parts of Vulkan. Relatively speaking, of
course. Memory management, devices, synchronization, and resources. I'll touch
on work submission, but that really comes later.

## The Instance

The first piece of Vulkan that any user will interact with is the *instance*,
the concrete object representing... well, an instance. A handle to the API.

As for why Instance objects exist, I could read the rationale but I'm not going
to. I'm going to guess and say that it's probably partly so that individual API
consumers have some kind of identifier inside the driver, and also (and this is
more concrete) because the Vulkan spec has the concept of a *loader*, which is
the actual libvulkan library you link to, which both binds you to a concrete
implementation (a GPU driver) and can insert *layers* between you and that
implementation at your or the user's request, like the Vulkan Validation Layers
or something like
[MangoHud](https://github.com/flightlessmango/MangoHud). Instance Creation is
where you get to request layers, as well as standardized API extensions you may
want or need, like the actual ability to render something to a screen. We'll get
to that...

## Devices, Physical and Virtual

The next step in bringing up a Vulkan app is to get your hands on a physical
device handle. This could be a GPU, or something else. But realistically it's
going to be a GPU, or a software emulation thereof. You don't typically interact
with a physical device, but instead with a virtual device, from which you can
request various features and extensions (like geometry and tesselation shaders,
anisotropic filtering, and raytracing extensions...). A virtual device needn't
necessarily map to a single a physical device, if for example you're using
SLI/Crossfire or a similar configuration. But in most cases it probably will:
Almost nobody uses SLI/Crossfire anymore, manufacturers don't care, and the
system was never quite as useful as manufacterers sold it as being. So if you
really need this feature, it's more likely you're doing GPGPU work that's above
my paygrade.

## Queues

Vulkan is a strictly asynchronous API: Work on the CPU continues after a job is
submitted to GPU. Furthermore, you're responsible for a lot of the
synchronization, both between on-GPU jobs (some out-of-order execution is
permitted by spec, although not as much as there might be, thank god) and
between the CPU and the GPU.

As such, Vulkan exposes *queues* to you, which are exactly what they sound like:
work submission queues for the GPU. Different queues may only accept certain
kinds of work: some queues may only accept compute loads, others only graphics,
etc.

For synchronization between queues, between operations, and between the CPU and
GPU, Vulkan provides both binary and monotonically increasing semaphores
(Between different GPU operations), fence barriers (between CPU and GPU), events
(fine-grained intra-queue and host-queue synchronization), pipeline barriers,
and the ability to wait for the queue or device to become idle (don't do this
unless you're tearing down your instance and associated context: You're letting
the pipeline idle and wasting throughput otherwise). If you're familiar with
multithreaded synchronization at all, or with out-of-order execution, these
concepts should be pretty familiar to you. You can read the spec for additional
information, and information about exactly what the synchronization guarantees
are and when you need to add your own synchronization.

## Memory Allocation

Vulkan is a low-level API, and low-level, as usual, means you have to manage
your own memory. You are responsible for allocating and freeing all your
resources. And this includes GPU-side resources as well: If you want to load
some data onto the GPU, it's your job to allocate the memory for that
data. However, allocating on-GPU under Vulkan is not like memory managemant that
you may be familiar with coming from C. Under the Vulkan model, hardware doesn't
necessarily have a unified heap: GPUs can expose any number of heaps. Some of
these heaps might be local to the CPU, some might be local to the GPU. Some
might be accessible the CPU, some might be GPU-internal (you won't be able to
copy data to them directly). Heaps can have restrictions as to what they can be
used for, and depending on what you want to do with the memory you're
allocating, you might need to choose another heap. If you need to store data
somewhere that's not CPU-accessible, you have to copy the data from the CPU to
somewhere that is, and then issue a command to copy the data where you want it
to go.

Also notable is that Vulkan *severely* limits the number of allocations you can
have (or at least, it only requires implementations to provide a very small
number). You shouldn't be calling vkAllocateMemory once for every single object
you need allocated on the GPU: You'll run out of allocations pretty
fast. Instead, you want to make a few large allocations, containing all the
memory you'll need, and parcel it out in chunks as needed.

This is pretty complicated, and most of time you don't need this much control
over your allocations. If you want to work with Vulkan but this whole memory
thing is giving you a headache, AMD wrote an stb-style single-header library
called [VMA](https://gpuopen.com/vulkan-memory-allocator/), which abstracts away
most of it for you. It's generously licensed, and while it is a C++ library, the
interface is in C so you don't need to sully your code in order to use it.

However, even if you are using VMA, it's worth being aware of all of this.

## Resources

Vulkan's "resources" are memory blocks that contain data. There are really only
two kinds: *buffers* and *images*. buffers and images are both blocks of data,
but they have different uses, although they do overlap. 

Buffers are always linear arrays, and don't have tiling, miplevels, or
layouts. As a result, while you can read and write texture data to buffers, even
in shaders, you probably don't want to. They're essentially just a bunch of
bytes. You can use a *buffer view* to change how you interpret the format of the
buffer: if it helps, think of a buffer view of a slice (a subsection of a buffer
with a start point and extent) that also determines the type the bytes are
interpreted as. The most common uses of buffers are as a staging area to copy
data to other parts of GPU memory, and as shader inputs in the form of vertex
buffers, index buffers, indirect buffers, uniform buffers, and other storage for
non-texture elements that describe what and how to draw. If you've worked with
OpenGL before, this should all be pretty easy to understand. But I didn't, so I
was confused. If you're not sure what all these uses are exactly, don't worry,
I'll talk about it more later.

Images are a bit more complex than buffers. They are one, two, or
three-dimensional arrays, which can also have mipmaps, a tiling (letting the GPU
layout the image in a nonlinear manner for more efficient access), and the
like. Images are accessed through *views*, which are very similar to buffer
views, but have some more options of the sort that are convenient if you're
working with... well, images in the colloquial sense, collections of pixels that
form a picture.

When images and buffers are created, they aren't actually backed by any
memory. Memory fitting the memory requirements of the resource has to be *bound*
to it after the fact and before you actually use the resource.

## The Window System

There's no standard way for Vulkan to interact with a window system. This makes
sense, because there's no standard window system, and no cross-platform way to
interact with them. Instead, there are a set of standard extensions for
interfacing with all the different window systems that exist across platforms,
as well one for actually rendering to a window across platforms, once you've
gotten a handle to one in a platform specific manner. In theory, there could be
an entirely conforming implementation of Vulkan that provided none of this, and
didn't actually permit you to render to a screen at all. In practice, this is
extremely unlikely.

If wrangling a bunch of different platform-specific APIs sounds like a miserable
thing to do and you just want to get on with writing your app, that's good. It
means you're still sane. Fortunately, it's not your job to do any of this stuff:
You should be using an external library like [SDL2](https://www.libsdl.org/) or
[GLFW](https://www.glfw.org/). They handle this stuff and a lot of other
cross-platform concerns for you, probably better than you would have if left to
your own devices. Personally, I'll endorse SDL: It's bigger, but it provides
more utility and it's just generally rock-solid. But GLFW is a fine choice too.

Moving on, the first object you'll acquire in order to images onto the screen
is...

### The Surface

A Vulkan Surface is an abstract representation of a window, your app in
fullscreen, or really anything you can present images to. It has a size, which
may change over time if it gets resized (and you'll need to rebuild parts of
your pipeline if it changes, so watch out!). You present to a surface (ie,
actually put images on the screen) by issuing presentation commands to a queue
that supports them. Any queue that you can issue graphics commands to probably
will support such commands, but this isn't guaranteed: design around the
assumtion that your app might be on a system where this is not the case.

You can't actually do a whole lot with a surface directly. Most of the time
you'll actually be dealing with...

### The Swapchain

The Swapchain is just a collection of images that are controlled and allocated
by the driver. Different hardware may have different minimum and maximum numbers
of images required to be in the swapchain. You can request any number in between
the minimum and maximum (naturally).

As for why the swapchain exists, it's because you can't draw to the memory that
is currently being sent out to the screen. The swapchain provides the extra
images you'll need, and lets you *swap* between them, changing what's on
screen. You can change how and when this swapping occurs to implement vsync,
triple-buffering, and even immediate display (which will cause tearing).

# Rendering

At last, we arrive at the truly *complicated* part of the overview, and the most
complex part of Vulkan itself: Rendering. This only makes sense, rendering is
the primary focus of Vulkan. But there's a lot here, and it's pretty
complicated. Here we go...

## The Render Pass

The Render Pass object answers the broad question "what will this rendering
operation *do*?" It does not record commands, nor specifics. But it does address
the generalities: How many resources (dubbed "attachments" in this context) does
the render pass need? Of what kind? What will it do with them? What state should
they be in at the start and end of the subpass? Speaking of which Render passes
may be broken into subpasses, which can have inter-dependencies. This may be
useful for adding, say, post-processing, doing dynamic lighting, and other cases
where you will have multiple passes. However, subpasses are sharply limited:
Work for a given pixel in a subpass on an image that was touched by a prior
subpass can only read information about *that specific pixel*. So, for example,
blur effects cannot be implemented by a subpass.

Note that not all resources required during rendering are described in the
render pass. Essentially, only actual ouputs are considered. And of course,
shaders can do essentially arbitrary things, which muddy the waters further. So
most of the time, the attachements to render pass will be your output color
buffer, your Z-buffer, and your depth stencil (assuming you're using the latter
two at all..). Also note that the render pass does not specify the particular
set of attachments to be used in rendering, only what will be done with them and
how they should be initialized. The object containing the actual attachments to
bind to the render pass (which are bound at command recording time) is called
the *Frame Buffer*. Which given that it's where the output of a render will be
stored is probably technically correct but just *feels so wrong* when I say it.

## The Pipeline

If the render pass answers the question of *what* we're doing, the pipeline
object informs Vulkan of the nitty-gritty of *how* we're doing it. This is one
massive static object that provides the system with a ton of information about
how you're rendering stuff. What shaders are you using? If you're using a depth
stencil, what are you doing with it? Are you doing backface culling? Using a
depth clamp? How are you culling? Are you drawing points? lines? triangles?
Strips? Fans? Do you want to make something about this pipeline configurable at
render-time? (there are very few things you actually *can* configure at
render-time...)? Basically, the pipeline contains all the information about how
we intend to get from a list of points and a bunch of resources to an image
onscreen.

It's worth noting that you can absolutely use multiple pipelines when rendering
a single subpass. In fact, with complex scenes, this may be necessary: If you're
using different shaders or different options for different parts of a scene,
you'll need to use a different pipeline. Creating pipelines is relatively
expensive, so you should plan for this ahead of time and build all the different
pipelines you'll need as early as you can.

## Shaders

If you're an artist, you might assume shaders control how images are shaded: how
they look onscreen, light and shadows and all that. That is, in a sense, part of
the task shaders have, but shaders do a lot more than that.

What shaders really are is code that runs our GPU. All the parts of the pipeline
that are "programmable" are controlled by shaders. Shaders control how
individual pixels are rendered, but they also take input vertices and transform
them to put them where they'll be onscreen. Typically, you'll send your models
to the vertex shader, along with a matrix that will rotate and translate it to
where it should be in the scene, and have the shader (and thus GPU) handle it,
rather than working out that positioning on the CPU. The vertex shader also
handles projection: making images appear according to correct perspective and
making sure that your FOV setting actually affects what's onscreen is partly the
vertex shader's job. And shaders are also responsible for doing texture mapping
(the fragment shader handles this). On newer GPUs (most of the ones that support
Vulkan), additional shader stages exist that let you subdivide geometry and even
add new geometry. This lets you do level-of-detail (LOD) adjustment, changing
the number of polygons and complexity of a model depending on how far from the
camera it is.

In Vulkan, shaders are pre-compiled to a bytecode format called SPIR-V, which
typically has a .spv file extension. You then create shader module objects from
the raw bytecode, which can be added to pipelines. Shaders can be written in
anything that can be compiled to spv, but there aren't terribly many languages
that support this right now. Most commonly (and by tradition) shaders are
written in GLSL, the OpenGL Shading Language. There are many references and
guides to learning GLSL online, although not all the information available
applies to Vulkan.

## Descriptors

Descriptors are sort of handles, resource identifiers that let shaders freely
access that resource. Much like file descriptors.

You don't need descriptors for every kind of resource a shader touches. In
particular, shader inputs and outputs don't require descriptors, so vertex
buffers being fed to the vertex shader don't need a descriptor, and neither do
the framebuffers that the fragment shader writes to. Essentially, descriptors
bind uniforms, storage buffers, samplers, and images. Descriptors come in a set,
which is basically just an array of descriptors: you can't create single
descriptors on your own. You can have multiple descriptor sets if you want. Each
descriptor in a set has a binding number, and can be referred to uniquely by its
set number and binding number: This is how the object referenced by the
descriptor is bound in GLSL (using layout qualifiers). Before creating your
descriptor set, you'll need to created a layout object that describes what will
be in it. You also must provide information about how many sets you will used
and how they are laid out to your pipeline when that's being created.

When the time comes to actually create your descriptor set, you can't allocate
it directly: you need to allocate it from a pool object. I forget why you need
to do this, but I think it has something to do with providing good
multithreading support. There must be some state involved with creating sets or
something that the instance needs to track.

After a descriptor set is created, it has to be written to with information:
most importantly, what the heck each descriptor in the set is pointing to and
how big that thing is. Finally, in order to use a descriptor set during drawing,
you'll need to bind it before issuing a draw command during command buffer
recording. And that's more or less all there is to it.

## Samplers

I casually mentioned samplers above. These are objects that get used in your
fragment shader. They contain information about how to address textures when
you're mapping them. This is where you can enable and disable bilinear or
trilinear filtering, specify whether the texture should repeat or not, how much
(if any) anisotropic filtering to have, how the texture is addressed, lod
biases, and in general just about 90% of the information that a user might
configure in your average PC game options menu.

Samplers can be passed to the shader on their own, or be linked to an image so
that they can only be used by that image (this is called a "combined image
sampler" for reasons that are pretty obvious). Combined imaged samplers might be
slower than samplers on their own. They also might be faster. The spec does not
say, although I read a forum post by a guy that says that combined samplers
perform better on at least one popular manufacturer's graphics hardware. This
would't surprise me if it were true: graphics hardware manufacturers
suck. Especially Intel: friends don't let friends use Intel HD graphics.

## Commands

At last we arrive at the big, important part of drawing: commands. Commands are
the glue that brings everything else together. It's commands that move through
render passes and subpasses, bind descriptors, set push constants, copy data,
bind the vertex buffers, bind the pipeline, and actually draw whatever piece of
crap you're putting on screen and calling a videogame (If you're not writing a
videogame, my apologies). To be more precise, commands do all the big heavy work
that happens on the GPU: all the stuff that needs to be submitted to queue, will
be done asynchronously, and might take a while. (Well, there are also things
called queries. I won't talk about them right now, but they're more or less what
they sound like).

Commands are packaged up into command buffers, which are what you record a set
of commands to send to the GPU in, and what you actually send to the queue. Most
of the time, command buffers can be recorded once and sent as many times as you
like, although it's rare to actually be able to do so, as commands are often
specific to the actual frame you're drawing. Like descriptor sets, command
buffers aren't directly allocated: they're allocated from pool
objects. Something something multithreading.

In addition to normal command buffers, Vulkan provides something called
*secondary* command buffers. These command buffers can't be directly submitted,
but can be called from other command buffers. So if you have a batch of commands
that only need to be recorded once while the rest of your command buffer will
need to be rerecorded every frame, these can be helpful.

Vulkan's command buffers are relatively cheap to record compared to their
equivalents in other APIs, but recording a command buffer should still be
considered an expensive operation operation.

# Conclusion?

I've now covered almost every object in Vulkan. Not quite all of them: I've
ignored various nooks and crannys, especially those pertaining to compute (sorry
GPGPU people, but... admit it, you don't actually *care* about our graphics
API...). At the very least, I've found it helpful for me. I don't know about
you. I certainly don't recommend this as your only resource on Vulkan. The
Vulkan Tutorial is excellent, even for the C++-averse like me, and the specs
are more lucid than some I've read (WebAssembly...).

If I make a part two, I'll be talking a bit about computer graphics from first
principles: annoying math, rasterization, the sort of stuff that carries across
APIs. Because it's been a right headache to understand, so I'm going to
rubber-duck the hell out of it.
